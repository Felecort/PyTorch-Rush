{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q2gQc6O_uELi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn # NN networks (CNN, RNN, losses)\n",
        "import torch.optim as optim # Aptimizers (Adam, Adadelta, Adagrad)\n",
        "import torch.nn.functional as F # Activarions func (ReLU, Sigmoid) also included in nn\n",
        "from torch.utils.data import DataLoader # Dataset manager\n",
        "import torchvision.datasets as datasets # Datasets\n",
        "import torchvision.transforms as transforms # Transformation to datasets\n",
        "import torchvision\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "knlitASIyXEW"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparametrs\n",
        "in_channels = 3\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epoches = 1\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ef-O9I0MztiD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_dataset = datasets.CIFAR10(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Projects\\PythonProjects\\_venvs\\ml_venv_p39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "d:\\Projects\\PythonProjects\\_venvs\\ml_venv_p39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained model\n",
        "model = torchvision.models.vgg16(pretrained=True)\n",
        "# print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What we did?\n",
        "# If we want to delete some layer of pretrained NN\n",
        "# we could create NN class, that return the input -> nothing do\n",
        "# We set the avgpool equal to our class, that return input \n",
        "model.avgpool = Identity()\n",
        "model.classifier = nn.Linear(512, 10)\n",
        "# print(model)\n",
        "model.to(device=device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 complete\n",
            "5 complete\n",
            "10 complete\n",
            "15 complete\n",
            "20 complete\n",
            "25 complete\n",
            "30 complete\n",
            "35 complete\n",
            "40 complete\n",
            "45 complete\n",
            "50 complete\n",
            "55 complete\n",
            "60 complete\n",
            "65 complete\n",
            "70 complete\n",
            "75 complete\n",
            "80 complete\n",
            "85 complete\n",
            "90 complete\n",
            "95 complete\n",
            "100 complete\n",
            "105 complete\n",
            "110 complete\n",
            "115 complete\n",
            "120 complete\n",
            "125 complete\n",
            "130 complete\n",
            "135 complete\n",
            "140 complete\n",
            "145 complete\n",
            "150 complete\n",
            "155 complete\n",
            "160 complete\n",
            "165 complete\n",
            "170 complete\n",
            "175 complete\n",
            "180 complete\n",
            "185 complete\n",
            "190 complete\n",
            "195 complete\n",
            "200 complete\n",
            "205 complete\n",
            "210 complete\n",
            "215 complete\n",
            "220 complete\n",
            "225 complete\n",
            "230 complete\n",
            "235 complete\n",
            "240 complete\n",
            "245 complete\n",
            "250 complete\n",
            "255 complete\n",
            "260 complete\n",
            "265 complete\n",
            "270 complete\n",
            "275 complete\n",
            "280 complete\n",
            "285 complete\n",
            "290 complete\n",
            "295 complete\n",
            "300 complete\n",
            "305 complete\n",
            "310 complete\n",
            "315 complete\n",
            "320 complete\n",
            "325 complete\n",
            "330 complete\n",
            "335 complete\n",
            "340 complete\n",
            "345 complete\n",
            "350 complete\n",
            "355 complete\n",
            "360 complete\n",
            "365 complete\n",
            "370 complete\n",
            "375 complete\n",
            "380 complete\n",
            "385 complete\n",
            "390 complete\n",
            "395 complete\n",
            "400 complete\n",
            "405 complete\n",
            "410 complete\n",
            "415 complete\n",
            "420 complete\n",
            "425 complete\n",
            "430 complete\n",
            "435 complete\n",
            "440 complete\n",
            "445 complete\n",
            "450 complete\n",
            "455 complete\n",
            "460 complete\n",
            "465 complete\n",
            "470 complete\n",
            "475 complete\n",
            "480 complete\n",
            "485 complete\n",
            "490 complete\n",
            "495 complete\n",
            "500 complete\n",
            "505 complete\n",
            "510 complete\n",
            "515 complete\n",
            "520 complete\n",
            "525 complete\n",
            "530 complete\n",
            "535 complete\n",
            "540 complete\n",
            "545 complete\n",
            "550 complete\n",
            "555 complete\n",
            "560 complete\n",
            "565 complete\n",
            "570 complete\n",
            "575 complete\n",
            "580 complete\n",
            "585 complete\n",
            "590 complete\n",
            "595 complete\n",
            "600 complete\n",
            "605 complete\n",
            "610 complete\n",
            "615 complete\n",
            "620 complete\n",
            "625 complete\n",
            "630 complete\n",
            "635 complete\n",
            "640 complete\n",
            "645 complete\n",
            "650 complete\n",
            "655 complete\n",
            "660 complete\n",
            "665 complete\n",
            "670 complete\n",
            "675 complete\n",
            "680 complete\n",
            "685 complete\n",
            "690 complete\n",
            "695 complete\n",
            "700 complete\n",
            "705 complete\n",
            "710 complete\n",
            "715 complete\n",
            "720 complete\n",
            "725 complete\n",
            "730 complete\n",
            "735 complete\n",
            "740 complete\n",
            "745 complete\n",
            "750 complete\n",
            "755 complete\n",
            "760 complete\n",
            "765 complete\n",
            "770 complete\n",
            "775 complete\n",
            "780 complete\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "for epoch in range(num_epoches):\n",
        "\n",
        "    for batch_ind, (data, targets) in enumerate(train_loader):\n",
        "        if batch_ind % 5 == 0:\n",
        "            print(f\"{batch_ind} complete\")\n",
        "        # Data on cude\n",
        "        data = data.to(device=device) # ([64, 1, 28, 28])\n",
        "        targets = targets.to(device=device)\n",
        "        \n",
        "        # Forawrd\n",
        "        scores = model(data) # Equal to model.forward(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient descent or adam step\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            \n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    print(f\"{float(num_correct) / float(num_samples)}\")\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VmFuLUS0-EVH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.62492\n",
            "0.6174\n"
          ]
        }
      ],
      "source": [
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "1_simple_fullynet",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ml_venv_p39')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "4768d2f058c14b479e138ff57b58bdd686493c6bb5dedc1b4ea8047c7aed7072"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
